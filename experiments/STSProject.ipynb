{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alberto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/alberto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alberto/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alberto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alberto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/alberto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alberto/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alberto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import clean_func, spacy_preprocess_reviews\n",
    "from src.sts import STSAnalyzer\n",
    "from src.models import BaselineModel\n",
    "import numpy as np\n",
    "from nltk.metrics import jaccard_distance\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "# IMport partial\n",
    "import nltk\n",
    "\n",
    "from functools import partial\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk import ngrams\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "from nltk import download\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "download('averaged_perceptron_tagger')\n",
    "download('wordnet')\n",
    "download('omw-1.4')\n",
    "download('punkt')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "# Load a spaCy model (supported languages are \"es\" and \"en\") \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# spaCy 3.x\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger')\n",
    "\n",
    "import textdistance\n",
    "from src.fe_utils import *\n",
    "\n",
    "prep_func = partial(spacy_preprocess_reviews, clean_func=clean_func, out_set=False)\n",
    "INPUT_FOLDER = \"input\"\n",
    "TRAIN_PATH = os.path.join(INPUT_FOLDER, \"train\")\n",
    "TEST_PATH = os.path.join(INPUT_FOLDER, \"test\")\n",
    "LS_FILES_TRAIN = [\"MSRpar\", \"MSRvid\", \"SMTeuroparl\"]\n",
    "LS_FILES_TEST = [\"MSRpar\", \"MSRvid\", \"SMTeuroparl\", \"surprise.SMTnews\", \"surprise.OnWN\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the course, different text cleaning and preprocessing techniques were introduced. In this section, we will use the functions that were introduced in the course to clean and preprocess the text data.\n",
    "\n",
    "Our cleaning function will perform the following steps:\n",
    "\n",
    "- Standardize time expressions converting hours in 24h format processing things like \"8:30\" and \"8.30\" as the same thing and converting \"8am\" to \"8:00\" and \"8pm\" to \"20:00\"\n",
    "- Delete grammatic abbreviations such as \"'s\", \"'d\", \"'ll\", ... (note that n't is not removed because it has impact in the meaning)\n",
    "- Delete mr., mrs., ms., ...\n",
    "- Convert indexin symbols such as <.idx> into words\n",
    "- Remove the points at the begining of a word\n",
    "- Remove the \",\" of thousands to standardize numbers\n",
    "- Transform percentages into unique symbol (e.g. 10 percent to 10_%)\n",
    "- Remove dollar signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def clean_func(text):\n",
    "\n",
    "    # Standardize the hours (1pm -> 13:00)\n",
    "    def convert_to_24_hour(match):\n",
    "        '''\n",
    "        Function to convert time to 24 hour format\n",
    "        Input: match - match object\n",
    "        Output: string with the time in 24 hour format\n",
    "        '''\n",
    "        time_str = match.groups()\n",
    "        hour, minute, am_pm = time_str[0], '' if time_str[1] is None else time_str[1] ,  re.sub('\\.', '', '' if time_str[2] is None else time_str[2])\n",
    "        # print(time_str)\n",
    "        if am_pm == 'pm':\n",
    "            if minute == '':\n",
    "                return str(int(hour) + 12) + ':00'\n",
    "            else:\n",
    "                return str(int(hour) + 12) + ':' + minute\n",
    "        elif am_pm == 'am':\n",
    "            if hour == '12':\n",
    "                return \"00\" + ':' + minute\n",
    "            elif minute == '':\n",
    "                return hour + ':00'\n",
    "            else:\n",
    "                return hour + ':' + minute\n",
    "        else:\n",
    "            if minute == '':\n",
    "                return hour + ':00'\n",
    "            else:\n",
    "                return hour + ':' + minute\n",
    "\n",
    "    hour_pattern_1 = re.compile(r'(?<=\\s)(\\d+)\\s?()([ap]\\.?m\\.?)')\n",
    "    hour_pattern_2 = re.compile(r'(\\d+)[:\\.](\\d+)\\s?([ap]\\.?m\\.?)')\n",
    "    hour_pattern_3 = re.compile(r'(\\d+)\\s?[h:]\\s?(\\d+)\\s?([ap]\\.?m\\.?)?')\n",
    "    ls_patterns = [hour_pattern_1, hour_pattern_2, hour_pattern_3]\n",
    "    for pat in ls_patterns:\n",
    "        text = re.sub(pat, convert_to_24_hour, text)\n",
    "\n",
    "    # Delete rests of abbreviated particles\n",
    "    particle_pattern = re.compile(r\"\\b(?:'s|'d|'ll|'m|'re|'ve)\\b\", re.IGNORECASE)\n",
    "    formatted_text = re.sub(particle_pattern, '', text)\n",
    "\n",
    "    # Delete rests of abbreviated words to refer people\n",
    "    mr_ms_pattern = re.compile(r\"\\b(mr.|mr|ms|ms.|mss.)\\b\", re.IGNORECASE)\n",
    "    formatted_text = re.sub(mr_ms_pattern, '', formatted_text)\n",
    "\n",
    "    # Convert indexes to a single word\n",
    "    idx_pattern = re.compile(r'<\\W*(\\w+)=*(\\w*)>', re.IGNORECASE)\n",
    "    formatted_text = re.sub(idx_pattern, r'\\1\\2', formatted_text)\n",
    "    \n",
    "    # Remove the point from words starting with it\n",
    "    start_point_pattern = re.compile(r'(\\W)\\.(\\w+)', re.IGNORECASE)\n",
    "    formatted_text = re.sub(start_point_pattern, r'\\2', formatted_text)\n",
    "\n",
    "    # Remove the comma from thousands (standardize)\n",
    "    thousands_pattern = re.compile(r'(\\d+),(\\d+)', re.IGNORECASE)\n",
    "    formatted_text = re.sub(thousands_pattern, r'\\1\\2', formatted_text)\n",
    "\n",
    "    # Transform percentages to a single word\n",
    "    percent_pattern = re.compile(r'(\\d+)\\s*per\\s*cent', re.IGNORECASE)\n",
    "    formatted_text = re.sub(percent_pattern, r'\\1_%', formatted_text)\n",
    "    \n",
    "    # Remove the dollar sign from money\n",
    "    money_pattern = re.compile(r'\\$', re.IGNORECASE)\n",
    "    formatted_text = re.sub(money_pattern, '' , formatted_text)\n",
    "    \n",
    "    # ls_removable_symbols = ['<', '>']\n",
    "    # for symbol in ls_removable_symbols:\n",
    "    #     formatted_text = formatted_text.replace(symbol, '')\n",
    "        \n",
    "    return formatted_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is clean, preprocessing can be performed. In this case, we will use the following preprocessing steps:\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Stopwords removal\n",
    "- Punctuation removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
